{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to generate batches from a dataset and work with batch components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "# the following line is not required if BatchFlow is installed as a python package.\n",
    "sys.path.append(\"../..\")\n",
    "from batchflow import Dataset, DatasetIndex, Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of items in the dataset\n",
    "NUM_ITEMS = 10\n",
    "# number of items in a batch when iterating\n",
    "BATCH_SIZE = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dataset is defined by an index (a sequence of item ids) and a batch class (see [the documentation for details](https://analysiscenter.github.io/batchflow/intro/dataset.html))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the simplest case an index is a natural sequence 0, 1, 2, 3, ...\n",
    "\n",
    "So all you need to define the index is just a number of items in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(index=NUM_ITEMS, batch_class=Batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The dataset index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [the documentation](https://analysiscenter.github.io/batchflow/intro/index.html) for more info about how to create an index which fits your needs.\n",
    "\n",
    "Here are the most frequent use cases:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    client_index = DatasetIndex(my_client_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    images_index = FilesIndex(path=\"/path/to/images/*.jpg\", no_ext=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterate with gen_batch(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`gen_batch` is a python [generator](https://wiki.python.org/moin/Generators)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0  contains items [0 1 2]\n",
      "batch 1  contains items [3 4 5]\n",
      "batch 2  contains items [6 7 8]\n",
      "batch 3  contains items [9]\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(dataset.gen_batch(BATCH_SIZE, n_epochs=1)):\n",
    "    print(\"batch\", i, \" contains items\", batch.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0  contains items [0 1 2]\n",
      "batch 1  contains items [3 4 5]\n",
      "batch 2  contains items [6 7 8]\n",
      "batch 3  contains items [9 0 1]\n",
      "batch 4  contains items [2 3 4]\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(dataset.gen_batch(BATCH_SIZE, n_iters=5)):\n",
    "    print(\"batch\", i, \" contains items\", batch.indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### drop_last=True skips the last batch if it contains fewer than BATCH_SIZE items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0  contains items [0 1 2]\n",
      "batch 1  contains items [3 4 5]\n",
      "batch 2  contains items [6 7 8]\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(dataset.gen_batch(BATCH_SIZE, n_epochs=1, drop_last=True)):\n",
    "    print(\"batch\", i, \" contains items\", batch.indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### shuffle permutes items across batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0  contains items [8 6 4]\n",
      "batch 1  contains items [3 2 9]\n",
      "batch 2  contains items [5 0 7]\n",
      "batch 3  contains items [5 0 1]\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(dataset.gen_batch(BATCH_SIZE, n_iters=4, drop_last=True, shuffle=True)):\n",
    "    print(\"batch\", i, \" contains items\", batch.indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell above multiple times to see how batches change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle can be bool, int (seed number) or a RandomState object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0  contains items [4 0 7]\n",
      "batch 1  contains items [5 8 3]\n",
      "batch 2  contains items [1 6 9]\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(dataset.gen_batch(BATCH_SIZE, n_epochs=1, drop_last=True, shuffle=123)):\n",
    "    print(\"batch\", i, \" contains items\", batch.indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell above multiple times to see that batches stay the same across runs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterate with next_batch(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While `gen_batch` is a generator, `next_batch` is an ordinary method.\n",
    "Most of the time you will use `gen_batch`, but for a deeper control over training and a more sophisticated finetuning `next_batch` might be more convenient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If too many iterations are made, `StopIteration` will be raised.\n",
    "\n",
    "Check that there are `NUM_ITEMS * 3` iterations (i.e. 3 epochs), but `n_epochs=2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 1 contains items [5 1 4]\n",
      "batch 2 contains items [3 0 7]\n",
      "batch 3 contains items [8 6 9]\n",
      "batch 4 contains items [0 4 5]\n",
      "batch 5 contains items [6 8 9]\n",
      "batch 6 contains items [2 3 7]\n",
      "got StopIteration\n"
     ]
    }
   ],
   "source": [
    "for i in range(NUM_ITEMS * 3):\n",
    "    try:\n",
    "        batch = dataset.next_batch(BATCH_SIZE, shuffle=True, n_iters=6, drop_last=True)\n",
    "        print(\"batch\", i + 1, \"contains items\", batch.indices)\n",
    "    except StopIteration:\n",
    "        print(\"got StopIteration\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And finally with shuffle=True, n_epochs=None and a variable batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do not forget to reset iterator to start `next_batch`'ing from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.reset_iter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`n_epochs=None` allows for infinite iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 1 contains items [4 3 7]\n",
      "batch 2 contains items [2 5 6 8 1]\n",
      "batch 3 contains items [5 0 8 2 4]\n",
      "batch 4 contains items [9 1 6]\n",
      "batch 5 contains items [8 3 7 4]\n",
      "batch 6 contains items [6 9 1 2]\n",
      "batch 7 contains items [4 3 7]\n",
      "batch 8 contains items [5 9 8 0 6]\n",
      "batch 9 contains items [7 5 4 2 6]\n",
      "batch 10 contains items [9 8 0]\n",
      "batch 11 contains items [2 0 3 8]\n",
      "batch 12 contains items [4 1 9 6]\n",
      "batch 13 contains items [6 4 9]\n"
     ]
    }
   ],
   "source": [
    "for i in range(int(NUM_ITEMS * 1.3)):\n",
    "    batch = dataset.next_batch(BATCH_SIZE + (-1)**i * i % 3, shuffle=True, n_epochs=None, drop_last=True)\n",
    "    print(\"batch\", i + 1, \"contains items\", batch.indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a deeper understanding of `drop_last` read [very important notes in the API](https://analysiscenter.github.io/batchflow/api/batchflow.index.html#batchflow.DatasetIndex.next_batch)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For illustrative purposes let's create a small array which will serve as a raw data source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   1,   2],\n",
       "       [100, 101, 102],\n",
       "       [200, 201, 202],\n",
       "       [300, 301, 302],\n",
       "       [400, 401, 402],\n",
       "       [500, 501, 502],\n",
       "       [600, 601, 602],\n",
       "       [700, 701, 702],\n",
       "       [800, 801, 802],\n",
       "       [900, 901, 902]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.arange(NUM_ITEMS).reshape(-1, 1) * 100 + np.arange(3).reshape(1, -1)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data into a batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading data is available as `batch.data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch contains items with indices [0 1 2]\n",
      "and batch data is\n",
      "[[  0   1   2]\n",
      " [100 101 102]\n",
      " [200 201 202]]\n",
      "\n",
      "batch contains items with indices [3 4 5]\n",
      "and batch data is\n",
      "[[300 301 302]\n",
      " [400 401 402]\n",
      " [500 501 502]]\n",
      "\n",
      "batch contains items with indices [6 7 8]\n",
      "and batch data is\n",
      "[[600 601 602]\n",
      " [700 701 702]\n",
      " [800 801 802]]\n",
      "\n",
      "batch contains items with indices [9]\n",
      "and batch data is\n",
      "[[900 901 902]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for batch in dataset.gen_batch(BATCH_SIZE, n_epochs=1):\n",
    "    batch = batch.load(src=data)\n",
    "    print(\"batch contains items with indices\", batch.indices)\n",
    "    print('and batch data is')\n",
    "    print(batch.data)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You can easily iterate over batch items too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch contains\n",
      "[0 1 2]\n",
      "[100 101 102]\n",
      "[200 201 202]\n",
      "\n",
      "batch contains\n",
      "[300 301 302]\n",
      "[400 401 402]\n",
      "[500 501 502]\n",
      "\n",
      "batch contains\n",
      "[600 601 602]\n",
      "[700 701 702]\n",
      "[800 801 802]\n",
      "\n",
      "batch contains\n",
      "[900 901 902]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for batch in dataset.gen_batch(BATCH_SIZE, n_epochs=1):\n",
    "    batch = batch.load(src=data)\n",
    "    print(\"batch contains\")\n",
    "    for item in batch:\n",
    "        print(item)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not infrequently, the batch stores a more complex data structures, e.g. features and labels or images, masks, bounding boxes and labels. To work with these you might employ data components. Just define a property as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyBatch(Batch):\n",
    "    components = 'features', 'labels'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate some random data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_array = np.arange(NUM_ITEMS).reshape(-1, 1) * 100 + np.arange(3).reshape(1, -1)\n",
    "labels_array = np.random.choice(10, size=NUM_ITEMS)\n",
    "data = features_array, labels_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create a dataset (`preloaded` handles data loading from data stored in memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(index=NUM_ITEMS, batch_class=MyBatch, preloaded=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since components are defined, you can address them as batch and even item attributes (they are created and loaded automatically)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0  contains items [0 1 2]\n",
      "and batch data consists of features:\n",
      "[[  0   1   2]\n",
      " [100 101 102]\n",
      " [200 201 202]]\n",
      "and labels: [5 4 4]\n",
      "\n",
      "batch 1  contains items [3 4 5]\n",
      "and batch data consists of features:\n",
      "[[300 301 302]\n",
      " [400 401 402]\n",
      " [500 501 502]]\n",
      "and labels: [3 6 9]\n",
      "\n",
      "batch 2  contains items [6 7 8]\n",
      "and batch data consists of features:\n",
      "[[600 601 602]\n",
      " [700 701 702]\n",
      " [800 801 802]]\n",
      "and labels: [6 6 5]\n",
      "\n",
      "batch 3  contains items [9]\n",
      "and batch data consists of features:\n",
      "[[900 901 902]]\n",
      "and labels: [2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(dataset.gen_batch(BATCH_SIZE, n_epochs=1)):\n",
    "    print(\"batch\", i, \" contains items\", batch.indices)\n",
    "    print(\"and batch data consists of features:\")\n",
    "    print(batch.features)\n",
    "    print(\"and labels:\", batch.labels)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You can iterate over batch items and change them on the fly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0\n",
      "item features: [0 1 2]     item label: 5\n",
      "item features: [100 101 102]     item label: 4\n",
      "item features: [200 201 202]     item label: 4\n",
      "\n",
      "You can change batch data, even scalars.\n",
      "New batch features:\n",
      " [[1000 1001 1002]\n",
      " [1100 1101 1102]\n",
      " [1200 1201 1202]]\n",
      "and labels: [105 104 104]\n",
      "\n",
      "Batch 1\n",
      "item features: [300 301 302]     item label: 3\n",
      "item features: [400 401 402]     item label: 6\n",
      "item features: [500 501 502]     item label: 9\n",
      "\n",
      "You can change batch data, even scalars.\n",
      "New batch features:\n",
      " [[1300 1301 1302]\n",
      " [1400 1401 1402]\n",
      " [1500 1501 1502]]\n",
      "and labels: [103 106 109]\n",
      "\n",
      "Batch 2\n",
      "item features: [600 601 602]     item label: 6\n",
      "item features: [700 701 702]     item label: 6\n",
      "item features: [800 801 802]     item label: 5\n",
      "\n",
      "You can change batch data, even scalars.\n",
      "New batch features:\n",
      " [[1600 1601 1602]\n",
      " [1700 1701 1702]\n",
      " [1800 1801 1802]]\n",
      "and labels: [106 106 105]\n",
      "\n",
      "Batch 3\n",
      "item features: [900 901 902]     item label: 2\n",
      "\n",
      "You can change batch data, even scalars.\n",
      "New batch features:\n",
      " [[1900 1901 1902]]\n",
      "and labels: [102]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(dataset.gen_batch(BATCH_SIZE, n_epochs=1)):\n",
    "    print(\"Batch\", i)\n",
    "    for item in batch:\n",
    "        print(\"item features:\", item.features, \"    item label:\", item.labels)\n",
    "\n",
    "    print()\n",
    "    print(\"You can change batch data, even scalars.\")\n",
    "    for item in batch:\n",
    "        item.features = item.features + 1000\n",
    "        item.labels = item.labels + 100\n",
    "    print(\"New batch features:\\n\", batch.features)\n",
    "    print(\"and labels:\", batch.labels)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For machine learning tasks you might need to split a dataset into train, test and validation parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.split(0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the dataset is split into train / test in 80/20 ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 2)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.train), len(dataset.test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.split([.6, .2, .2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 2, 2)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.train), len(dataset.test), len(dataset.validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset may be shuffled before splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.split(0.7, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 9, 7, 6, 5, 1, 3]), array([8, 4, 2]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.train.indices, dataset.test.indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, shuffle can be bool, int (seed number) or a RandomState object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__`dataset.train`__ and __`dataset.test`__ are also datasets so you can do anything you want including splitting them further into __`dataset.train.train`__, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the time, though, you will work with pipelines, not datasets.\n",
    "\n",
    "See [pipeline operations tutorial](./02_pipeline_operations.ipynb) for details or return to the [table of contents](./00_content.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
